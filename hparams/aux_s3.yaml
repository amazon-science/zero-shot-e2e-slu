# ############################################################################
# Model: E2E ASR with attention-based ASR
# Encoder: CRDNN model
# Decoder: GRU + beamsearch + RNNLM
# Tokens: BPE with unigram
# losses: CTC+ NLL
# Training: Librispeech 960h
# Authors:  Ju-Chieh Chou, Mirco Ravanelli, Abdel Heba, Peter Plantinga,
# Samuele Cornell 2020
# ############################################################################

# This yaml file is revised based on part of "recipes/LibriSpeech/seq2seq/train_BPE_1000.yaml"
# Seed needs to be set at top of yaml, before objects with parameters
seed: 2602
__set_seed: !apply:torch.manual_seed [!ref <seed>]
output_folder: !ref results/CRDNN_BPE_960h_LM/<seed>
wer_file: !ref <output_folder>/wer.txt
save_folder: !ref <output_folder>/save
train_log: !ref <output_folder>/train_log.txt

# Language model (LM) pretraining
# NB: To avoid mismatch, the speech recognizer must be trained with the same
# tokenizer used for LM training. Here, we download everything from the
# speechbrain HuggingFace repository. However, a local path pointing to a
# directory containing the lm.ckpt and tokenizer.ckpt may also be specified
# instead. E.g if you want to use your own LM / tokenizer.
pretrained_lm_tokenizer_path: speechbrain/asr-crdnn-rnnlm-librispeech

# Data files
data_folder: /home/jfhe/Documents/MountHe/jfhe/projects/speechbrain/datasets/LibriSpeech/LibriSpeech # !PLACEHOLDER # e,g./path/to/LibriSpeech
# noise/ris dataset will automatically be downloaded
data_folder_rirs: !ref <data_folder> # where to store noisy data for augment (change it if needed)

train_splits: ["train-clean-100", "train-clean-360", "train-other-500"]
dev_splits: ["dev-clean"]
test_splits: ["test-clean", "test-other"]
skip_prep: False
ckpt_interval_minutes: 15 # save checkpoint every N min
train_csv: !ref <output_folder>/train.csv
valid_csv: !ref <output_folder>/dev-clean.csv
test_csv:
   - !ref <output_folder>/test-clean.csv
   - !ref <output_folder>/test-other.csv


# dynamic batching parameters, if used
dynamic_batch_sampler:
   feats_hop_size: 0.01
   max_batch_len: 20000 # in terms of frames
   shuffle_ex: True
   batch_ordering: random
   num_buckets: 20

# Feature parameters
sample_rate: 16000
n_fft: 400
n_mels: 40

# Dataloader options
train_dataloader_opts:
   batch_size: !ref <batch_size>

valid_dataloader_opts:
   batch_size: !ref <batch_size>

test_dataloader_opts:
   batch_size: !ref <batch_size>


# This is the RNNLM that is used according to the Huggingface repository
# NB: It has to match the pre-trained RNNLM!!
lm_model: !new:speechbrain.lobes.models.RNNLM.RNNLM
   output_neurons: !ref <output_neurons>
   embedding_dim: !ref <emb_size>
   activation: !name:torch.nn.LeakyReLU
   dropout: 0.0
   rnn_layers: 2
   rnn_neurons: 2048
   dnn_blocks: 1
   dnn_neurons: 512
   return_hidden: True  # For inference

tokenizer: !new:sentencepiece.SentencePieceProcessor




